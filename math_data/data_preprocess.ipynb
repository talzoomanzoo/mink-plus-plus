{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjgwak/miniconda3/envs/uid_2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1590.56ba/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.90s/ shards]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/talzoomanzoo/brumo2025/commit/69ea201ce4a497bb3691701515aec8c652b118dc', commit_message='Upload dataset', commit_description='', oid='69ea201ce4a497bb3691701515aec8c652b118dc', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/talzoomanzoo/brumo2025', endpoint='https://huggingface.co', repo_type='dataset', repo_id='talzoomanzoo/brumo2025'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"./BRUMO\"):\n",
    "    os.makedirs(\"./BRUMO\")\n",
    "\n",
    "train_set = load_dataset(\"MathArena/brumo_2025\", split=\"train\")\n",
    "with open(\"./BRUMO/test.json\", \"w\") as f:\n",
    "    aime_train = []\n",
    "    for item in train_set:\n",
    "        aime_train.append({\n",
    "            'problem_idx': item['problem_idx'],\n",
    "            'Question': item['problem'],\n",
    "            'answer': item['answer'],\n",
    "            'problem_type': item['problem_type'],\n",
    "        })\n",
    "    json.dump(aime_train, f, ensure_ascii=False, indent=4)\n",
    "dataset = Dataset.from_list(aime_train)\n",
    "dataset.push_to_hub('talzoomanzoo/brumo2025', private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SUPERGPQA dataset from Hugging Face...\n",
      "Dataset loaded successfully! Found 26529 examples\n",
      "Dataset features: ['uuid', 'question', 'options', 'answer', 'answer_letter', 'discipline', 'field', 'subfield', 'difficulty', 'is_calculation']\n",
      "Converting to DataFrame...\n",
      "Saving to ./SUPERGPQA/original_data/SUPERGPQA.csv...\n",
      "\n",
      "Dataset Info:\n",
      "- Total rows: 26529\n",
      "- Columns: ['uuid', 'question', 'options', 'answer', 'answer_letter', 'discipline', 'field', 'subfield', 'difficulty', 'is_calculation']\n",
      "- File size: 18.79 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "#wrong format\n",
    "\n",
    "def download_superGPQA_to_csv(output_dir=\"./SUPERGPQA/original_data\", filename=\"SUPERGPQA.csv\"):\n",
    "    \"\"\"\n",
    "    Download the SUPERGPQA dataset from Hugging Face and save to CSV file.\n",
    "    \n",
    "    Args:\n",
    "        output_dir (str): Directory to save the CSV file\n",
    "        filename (str): Name of the CSV file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Load the dataset from Hugging Face\n",
    "        print(\"Loading SUPERGPQA dataset from Hugging Face...\")\n",
    "        dataset = load_dataset(\"m-a-p/SuperGPQA\", split=\"train\")\n",
    "        \n",
    "        print(f\"Dataset loaded successfully! Found {len(dataset)} examples\")\n",
    "        print(f\"Dataset features: {list(dataset.features.keys())}\")\n",
    "        \n",
    "        # Convert to pandas DataFrame\n",
    "        print(\"Converting to DataFrame...\")\n",
    "        df = dataset.to_pandas()\n",
    "        \n",
    "        # Save to CSV\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        print(f\"Saving to {output_path}...\")\n",
    "        df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "        \n",
    "        # Display some basic info about the dataset\n",
    "        print(\"\\nDataset Info:\")\n",
    "        print(f\"- Total rows: {len(df)}\")\n",
    "        print(f\"- Columns: {list(df.columns)}\")\n",
    "        print(f\"- File size: {os.path.getsize(output_path) / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading dataset: {e}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    download_superGPQA_to_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m formatted_answers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     41\u001b[0m correct_choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (label, answer) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(answers):\n\u001b[1;32m     43\u001b[0m     choice \u001b[38;5;241m=\u001b[39m choices[i]\n\u001b[1;32m     44\u001b[0m     formatted_answers\u001b[38;5;241m.\u001b[39mappend((choice, answer))\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "\"\"\"Data preprocess for SUPERGPQA\n",
    "Data Link: https://huggingface.co/datasets/Idavidrein/gpqa\n",
    "\"\"\"\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths to data\n",
    "split = 'diamond'  # diamond, main, extended\n",
    "data_path = f'./SUPERGPQA/original_data/SUPERGPQA.csv'\n",
    "output_path = f'./SUPERGPQA/train.json'\n",
    "\n",
    "# Define the keys we want to keep\n",
    "keys_to_keep = [\n",
    "    'uuid',\n",
    "    'question',\n",
    "    'options',\n",
    "    'discipline',\n",
    "    'field',\n",
    "    'subfield',\n",
    "    'difficulty',\n",
    "    'is_calculation'\n",
    "]\n",
    "\n",
    "filtered_data = []\n",
    "with open(data_path, mode='r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for idx, row in enumerate(tqdm(csv_reader), 0):\n",
    "        # Add id field\n",
    "        row['id'] = idx\n",
    "        # Create new dictionary with only desired keys\n",
    "        filtered_row = {key: row[key] for key in keys_to_keep}\n",
    "\n",
    "        # Extract answers and shuffle them\n",
    "        answers = filtered_row['options']\n",
    "\n",
    "        # Assign new choices A, B, C, D in order and determine the correct choice\n",
    "        choices = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
    "        formatted_answers = []\n",
    "        correct_choice = None\n",
    "        for i, (label, answer) in enumerate(answers):\n",
    "            choice = choices[i]\n",
    "            formatted_answers.append((choice, answer))\n",
    "            if label == 'Correct Answer':\n",
    "                correct_choice = choice\n",
    "\n",
    "        # Update the Question field\n",
    "        formatted_choices = \"\\n\".join([f\"({choice}) {answer}\" for choice, answer in formatted_answers])\n",
    "        filtered_row['Question'] = f\"{filtered_row['Question']} Choices:\\n{formatted_choices}\\n\"\n",
    "\n",
    "        # Add the Correct Choice field\n",
    "        filtered_row['Correct Choice'] = correct_choice\n",
    "\n",
    "        # Append the updated row to filtered_data\n",
    "        filtered_data.append(filtered_row)\n",
    "\n",
    "# Write the updated data to JSON\n",
    "with open(output_path, mode='w', encoding='utf-8') as json_file:\n",
    "    json.dump(filtered_data, json_file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 1548.28ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.48 shards/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/talzoomanzoo/olympiadbench/commit/58a71ac74e0cc78463bddb343bd98e0201ce90f1', commit_message='Upload dataset', commit_description='', oid='58a71ac74e0cc78463bddb343bd98e0201ce90f1', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/talzoomanzoo/olympiadbench', endpoint='https://huggingface.co', repo_type='dataset', repo_id='talzoomanzoo/olympiadbench'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"./OLYMPIADBENCH\"):\n",
    "    os.makedirs(\"./OLYMPIADBENCH\")\n",
    "\n",
    "test_set1 = load_dataset(\"Hothan/OlympiadBench\", 'OE_TO_maths_en_COMP', split=\"train\") #674\n",
    "test_set2 = load_dataset(\"Hothan/OlympiadBench\", 'OE_TO_maths_en_COMP', split=\"train\")\n",
    "\n",
    "with open(\"./OLYMPIADBENCH/test.json\", \"w\") as f:\n",
    "    aime_test = []\n",
    "    for item in test_set1:\n",
    "        aime_test.append({\n",
    "            'Question': item['question'],\n",
    "            'answer': item['final_answer'],\n",
    "        })\n",
    "    for item in test_set2:\n",
    "        aime_test.append({\n",
    "            'Question': item['question'],\n",
    "            'answer': item['final_answer'],\n",
    "        })\n",
    "\n",
    "    print(len(aime_test))\n",
    "    json.dump(aime_test, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "dataset = Dataset.from_list(aime_test)\n",
    "dataset.push_to_hub('talzoomanzoo/olympiadbench', private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 30/30 [00:00<00:00, 1305.43 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 3339.41ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.25s/ shards]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/talzoomanzoo/hmmt/commit/8602b342733ff6e33ab848936752a905478a5745', commit_message='Upload dataset', commit_description='', oid='8602b342733ff6e33ab848936752a905478a5745', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/talzoomanzoo/hmmt', endpoint='https://huggingface.co', repo_type='dataset', repo_id='talzoomanzoo/hmmt'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gsm8k\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import os\n",
    "\n",
    "#hmmt2025\n",
    "\n",
    "if not os.path.exists(\"./HMMT\"): \n",
    "    os.makedirs(\"./HMMT\")\n",
    "\n",
    "test_set = load_dataset(\"MathArena/hmmt_feb_2025\", split=\"train\")\n",
    "\n",
    "\n",
    "# Prepare test data\n",
    "hmmt_test = [] #30\n",
    "for item in test_set:\n",
    "    hmmt_test.append({\n",
    "        'Question': item['problem'],\n",
    "        'answer': item['answer'],\n",
    "    })\n",
    "\n",
    "# Save individual files\n",
    "with open(\"./HMMT/test.json\", \"w\") as f:\n",
    "    json.dump(hmmt_test, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Create DatasetDict with both splits\n",
    "dataset_dict = DatasetDict({\n",
    "    'test': Dataset.from_list(hmmt_test)\n",
    "})\n",
    "\n",
    "# Upload to hub with both splits\n",
    "dataset_dict.push_to_hub('talzoomanzoo/hmmt', private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjgwak/miniconda3/envs/uid_2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating test split: 100%|██████████| 272/272 [00:00<00:00, 6081.75 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 170.34ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.27s/ shards]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/talzoomanzoo/minervamath/commit/fc64b615c00e77b6c6bf9077815508779b5b7c04', commit_message='Upload dataset', commit_description='', oid='fc64b615c00e77b6c6bf9077815508779b5b7c04', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/talzoomanzoo/minervamath', endpoint='https://huggingface.co', repo_type='dataset', repo_id='talzoomanzoo/minervamath'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gsm8k\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"./MINERVA_MATH\"):\n",
    "    os.makedirs(\"./MINERVA_MATH\")\n",
    "\n",
    "test_set = load_dataset(\"math-ai/minervamath\", split=\"test\")\n",
    "\n",
    "\n",
    "# Prepare test data\n",
    "minerva_math_test = [] #272\n",
    "for item in test_set:\n",
    "    minerva_math_test.append({\n",
    "        'Question': item['question'],\n",
    "        'answer': item['answer'],\n",
    "    })\n",
    "\n",
    "# Save individual files\n",
    "with open(\"./MINERVA_MATH/test.json\", \"w\") as f:\n",
    "    json.dump(minerva_math_test, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Create DatasetDict with both splits\n",
    "dataset_dict = DatasetDict({\n",
    "    'test': Dataset.from_list(minerva_math_test)\n",
    "})\n",
    "\n",
    "# Upload to hub with both splits\n",
    "dataset_dict.push_to_hub('talzoomanzoo/minervamath', private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPQA dataset from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Old caching folder /home/mjgwak/.cache/huggingface/datasets/Idavidrein___gpqa/gpqa_diamond/0.0.0/90b8e5be2b1d3d2dbfe016cdab47981150600c4a for dataset gpqa exists but no data were found. Removing it. \n",
      "Generating train split: 100%|██████████| 198/198 [00:00<00:00, 6674.59 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully! Found 198 examples\n",
      "Dataset features: ['Pre-Revision Question', 'Pre-Revision Correct Answer', 'Pre-Revision Incorrect Answer 1', 'Pre-Revision Incorrect Answer 2', 'Pre-Revision Incorrect Answer 3', 'Pre-Revision Explanation', 'Self-reported question-writing time (minutes)', 'Question', 'Correct Answer', 'Incorrect Answer 1', 'Incorrect Answer 2', 'Incorrect Answer 3', 'Explanation', 'Revision Comments (from Question Writer)', 'Subdomain', \"Writer's Difficulty Estimate\", 'Extra Revised Question', 'Extra Revised Explanation', 'Extra Revised Correct Answer', 'Extra Revised Incorrect Answer 1', 'Extra Revised Incorrect Answer 2', 'Extra Revised Incorrect Answer 3', 'Non-Expert Validator Accuracy', 'Majority Non-Expert Vals Incorrect', 'Expert Validator Accuracy', 'Record ID', 'High-level domain', 'Question Writer', 'Feedback_EV_1', 'Validator Revision Suggestion_EV_1', 'Is First Validation_EV_1', 'Post hoc agreement_EV_1', 'Sufficient Expertise?_EV_1', 'Understand the question?_EV_1', 'Question Difficulty_EV_1', 'Validator Answered Correctly_EV_1', 'Self-reported time (minutes)_EV_1', 'Probability Correct_EV_1', 'Manual Correctness Adjustment_EV_1', 'Expert Validator_EV_1', 'Feedback_EV_2', 'Validator Revision Suggestion_EV_2', 'Is First Validation_EV_2', 'Post hoc agreement_EV_2', 'Sufficient Expertise?_EV_2', 'Understand the question?_EV_2', 'Question Difficulty_EV_2', 'Validator Answered Correctly_EV_2', 'Self-reported time (minutes)_EV_2', 'Probability Correct_EV_2', 'Manual Correctness Adjustment_EV_2', 'Expert Validator_EV_2', 'Feedback_NEV_1', 'Validator Answered Correctly_NEV_1', 'Explanation_NEV_1', 'Self-reported time (minutes)_NEV_1', 'Websites visited_NEV_1', 'Probability Correct_NEV_1', 'Manual Correctness Adjustment_NEV_1', 'Non-Expert Validator_NEV_1', 'Feedback_NEV_2', 'Validator Answered Correctly_NEV_2', 'Explanation_NEV_2', 'Self-reported time (minutes)_NEV_2', 'Websites visited_NEV_2', 'Probability Correct_NEV_2', 'Manual Correctness Adjustment_NEV_2', 'Non-Expert Validator_NEV_2', 'Feedback_NEV_3', 'Validator Answered Correctly_NEV_3', 'Explanation_NEV_3', 'Self-reported time (minutes)_NEV_3', 'Websites visited_NEV_3', 'Probability Correct_NEV_3', 'Manual Correctness Adjustment_NEV_3', 'Non-Expert Validator_NEV_3', 'Expert Validator Disagreement Category', 'Canary String']\n",
      "Converting to DataFrame...\n",
      "Saving to ./GPQA/original_data/gpqa_diamond.csv...\n",
      "\n",
      "Dataset Info:\n",
      "- Total rows: 198\n",
      "- Columns: ['Pre-Revision Question', 'Pre-Revision Correct Answer', 'Pre-Revision Incorrect Answer 1', 'Pre-Revision Incorrect Answer 2', 'Pre-Revision Incorrect Answer 3', 'Pre-Revision Explanation', 'Self-reported question-writing time (minutes)', 'Question', 'Correct Answer', 'Incorrect Answer 1', 'Incorrect Answer 2', 'Incorrect Answer 3', 'Explanation', 'Revision Comments (from Question Writer)', 'Subdomain', \"Writer's Difficulty Estimate\", 'Extra Revised Question', 'Extra Revised Explanation', 'Extra Revised Correct Answer', 'Extra Revised Incorrect Answer 1', 'Extra Revised Incorrect Answer 2', 'Extra Revised Incorrect Answer 3', 'Non-Expert Validator Accuracy', 'Majority Non-Expert Vals Incorrect', 'Expert Validator Accuracy', 'Record ID', 'High-level domain', 'Question Writer', 'Feedback_EV_1', 'Validator Revision Suggestion_EV_1', 'Is First Validation_EV_1', 'Post hoc agreement_EV_1', 'Sufficient Expertise?_EV_1', 'Understand the question?_EV_1', 'Question Difficulty_EV_1', 'Validator Answered Correctly_EV_1', 'Self-reported time (minutes)_EV_1', 'Probability Correct_EV_1', 'Manual Correctness Adjustment_EV_1', 'Expert Validator_EV_1', 'Feedback_EV_2', 'Validator Revision Suggestion_EV_2', 'Is First Validation_EV_2', 'Post hoc agreement_EV_2', 'Sufficient Expertise?_EV_2', 'Understand the question?_EV_2', 'Question Difficulty_EV_2', 'Validator Answered Correctly_EV_2', 'Self-reported time (minutes)_EV_2', 'Probability Correct_EV_2', 'Manual Correctness Adjustment_EV_2', 'Expert Validator_EV_2', 'Feedback_NEV_1', 'Validator Answered Correctly_NEV_1', 'Explanation_NEV_1', 'Self-reported time (minutes)_NEV_1', 'Websites visited_NEV_1', 'Probability Correct_NEV_1', 'Manual Correctness Adjustment_NEV_1', 'Non-Expert Validator_NEV_1', 'Feedback_NEV_2', 'Validator Answered Correctly_NEV_2', 'Explanation_NEV_2', 'Self-reported time (minutes)_NEV_2', 'Websites visited_NEV_2', 'Probability Correct_NEV_2', 'Manual Correctness Adjustment_NEV_2', 'Non-Expert Validator_NEV_2', 'Feedback_NEV_3', 'Validator Answered Correctly_NEV_3', 'Explanation_NEV_3', 'Self-reported time (minutes)_NEV_3', 'Websites visited_NEV_3', 'Probability Correct_NEV_3', 'Manual Correctness Adjustment_NEV_3', 'Non-Expert Validator_NEV_3', 'Expert Validator Disagreement Category', 'Canary String']\n",
      "- File size: 1.31 MB\n",
      "Loading GPQA dataset from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 448/448 [00:00<00:00, 10099.53 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully! Found 448 examples\n",
      "Dataset features: ['Pre-Revision Question', 'Pre-Revision Correct Answer', 'Pre-Revision Incorrect Answer 1', 'Pre-Revision Incorrect Answer 2', 'Pre-Revision Incorrect Answer 3', 'Pre-Revision Explanation', 'Self-reported question-writing time (minutes)', 'Question', 'Correct Answer', 'Incorrect Answer 1', 'Incorrect Answer 2', 'Incorrect Answer 3', 'Explanation', 'Revision Comments (from Question Writer)', 'Subdomain', \"Writer's Difficulty Estimate\", 'Extra Revised Question', 'Extra Revised Explanation', 'Extra Revised Correct Answer', 'Extra Revised Incorrect Answer 1', 'Extra Revised Incorrect Answer 2', 'Extra Revised Incorrect Answer 3', 'Non-Expert Validator Accuracy', 'Majority Non-Expert Vals Incorrect', 'Expert Validator Accuracy', 'Record ID', 'High-level domain', 'Question Writer', 'Feedback_EV_1', 'Validator Revision Suggestion_EV_1', 'Is First Validation_EV_1', 'Post hoc agreement_EV_1', 'Sufficient Expertise?_EV_1', 'Understand the question?_EV_1', 'Question Difficulty_EV_1', 'Validator Answered Correctly_EV_1', 'Self-reported time (minutes)_EV_1', 'Probability Correct_EV_1', 'Manual Correctness Adjustment_EV_1', 'Expert Validator_EV_1', 'Feedback_EV_2', 'Validator Revision Suggestion_EV_2', 'Is First Validation_EV_2', 'Post hoc agreement_EV_2', 'Sufficient Expertise?_EV_2', 'Understand the question?_EV_2', 'Question Difficulty_EV_2', 'Validator Answered Correctly_EV_2', 'Self-reported time (minutes)_EV_2', 'Probability Correct_EV_2', 'Manual Correctness Adjustment_EV_2', 'Expert Validator_EV_2', 'Feedback_NEV_1', 'Validator Answered Correctly_NEV_1', 'Explanation_NEV_1', 'Self-reported time (minutes)_NEV_1', 'Websites visited_NEV_1', 'Probability Correct_NEV_1', 'Manual Correctness Adjustment_NEV_1', 'Non-Expert Validator_NEV_1', 'Feedback_NEV_2', 'Validator Answered Correctly_NEV_2', 'Explanation_NEV_2', 'Self-reported time (minutes)_NEV_2', 'Websites visited_NEV_2', 'Probability Correct_NEV_2', 'Manual Correctness Adjustment_NEV_2', 'Non-Expert Validator_NEV_2', 'Feedback_NEV_3', 'Validator Answered Correctly_NEV_3', 'Explanation_NEV_3', 'Self-reported time (minutes)_NEV_3', 'Websites visited_NEV_3', 'Probability Correct_NEV_3', 'Manual Correctness Adjustment_NEV_3', 'Non-Expert Validator_NEV_3', 'Expert Validator Disagreement Category', 'Canary String']\n",
      "Converting to DataFrame...\n",
      "Saving to ./GPQA/original_data/gpqa_main.csv...\n",
      "\n",
      "Dataset Info:\n",
      "- Total rows: 448\n",
      "- Columns: ['Pre-Revision Question', 'Pre-Revision Correct Answer', 'Pre-Revision Incorrect Answer 1', 'Pre-Revision Incorrect Answer 2', 'Pre-Revision Incorrect Answer 3', 'Pre-Revision Explanation', 'Self-reported question-writing time (minutes)', 'Question', 'Correct Answer', 'Incorrect Answer 1', 'Incorrect Answer 2', 'Incorrect Answer 3', 'Explanation', 'Revision Comments (from Question Writer)', 'Subdomain', \"Writer's Difficulty Estimate\", 'Extra Revised Question', 'Extra Revised Explanation', 'Extra Revised Correct Answer', 'Extra Revised Incorrect Answer 1', 'Extra Revised Incorrect Answer 2', 'Extra Revised Incorrect Answer 3', 'Non-Expert Validator Accuracy', 'Majority Non-Expert Vals Incorrect', 'Expert Validator Accuracy', 'Record ID', 'High-level domain', 'Question Writer', 'Feedback_EV_1', 'Validator Revision Suggestion_EV_1', 'Is First Validation_EV_1', 'Post hoc agreement_EV_1', 'Sufficient Expertise?_EV_1', 'Understand the question?_EV_1', 'Question Difficulty_EV_1', 'Validator Answered Correctly_EV_1', 'Self-reported time (minutes)_EV_1', 'Probability Correct_EV_1', 'Manual Correctness Adjustment_EV_1', 'Expert Validator_EV_1', 'Feedback_EV_2', 'Validator Revision Suggestion_EV_2', 'Is First Validation_EV_2', 'Post hoc agreement_EV_2', 'Sufficient Expertise?_EV_2', 'Understand the question?_EV_2', 'Question Difficulty_EV_2', 'Validator Answered Correctly_EV_2', 'Self-reported time (minutes)_EV_2', 'Probability Correct_EV_2', 'Manual Correctness Adjustment_EV_2', 'Expert Validator_EV_2', 'Feedback_NEV_1', 'Validator Answered Correctly_NEV_1', 'Explanation_NEV_1', 'Self-reported time (minutes)_NEV_1', 'Websites visited_NEV_1', 'Probability Correct_NEV_1', 'Manual Correctness Adjustment_NEV_1', 'Non-Expert Validator_NEV_1', 'Feedback_NEV_2', 'Validator Answered Correctly_NEV_2', 'Explanation_NEV_2', 'Self-reported time (minutes)_NEV_2', 'Websites visited_NEV_2', 'Probability Correct_NEV_2', 'Manual Correctness Adjustment_NEV_2', 'Non-Expert Validator_NEV_2', 'Feedback_NEV_3', 'Validator Answered Correctly_NEV_3', 'Explanation_NEV_3', 'Self-reported time (minutes)_NEV_3', 'Websites visited_NEV_3', 'Probability Correct_NEV_3', 'Manual Correctness Adjustment_NEV_3', 'Non-Expert Validator_NEV_3', 'Expert Validator Disagreement Category', 'Canary String']\n",
      "- File size: 3.07 MB\n",
      "Loading GPQA dataset from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 546/546 [00:00<00:00, 9448.07 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully! Found 546 examples\n",
      "Dataset features: ['Pre-Revision Question', 'Pre-Revision Correct Answer', 'Pre-Revision Incorrect Answer 1', 'Pre-Revision Incorrect Answer 2', 'Pre-Revision Incorrect Answer 3', 'Pre-Revision Explanation', 'Self-reported question-writing time (minutes)', 'Question', 'Correct Answer', 'Incorrect Answer 1', 'Incorrect Answer 2', 'Incorrect Answer 3', 'Explanation', 'Revision Comments (from Question Writer)', 'Subdomain', \"Writer's Difficulty Estimate\", 'Extra Revised Question', 'Extra Revised Explanation', 'Extra Revised Correct Answer', 'Extra Revised Incorrect Answer 1', 'Extra Revised Incorrect Answer 2', 'Extra Revised Incorrect Answer 3', 'Non-Expert Validator Accuracy', 'Majority Non-Expert Vals Incorrect', 'Expert Validator Accuracy', 'Record ID', 'High-level domain', 'Question Writer', 'Feedback_EV_1', 'Validator Revision Suggestion_EV_1', 'Is First Validation_EV_1', 'Post hoc agreement_EV_1', 'Sufficient Expertise?_EV_1', 'Understand the question?_EV_1', 'Question Difficulty_EV_1', 'Validator Answered Correctly_EV_1', 'Self-reported time (minutes)_EV_1', 'Probability Correct_EV_1', 'Manual Correctness Adjustment_EV_1', 'Expert Validator_EV_1', 'Feedback_EV_2', 'Validator Revision Suggestion_EV_2', 'Is First Validation_EV_2', 'Post hoc agreement_EV_2', 'Sufficient Expertise?_EV_2', 'Understand the question?_EV_2', 'Question Difficulty_EV_2', 'Validator Answered Correctly_EV_2', 'Self-reported time (minutes)_EV_2', 'Probability Correct_EV_2', 'Manual Correctness Adjustment_EV_2', 'Expert Validator_EV_2', 'Feedback_NEV_1', 'Validator Answered Correctly_NEV_1', 'Explanation_NEV_1', 'Self-reported time (minutes)_NEV_1', 'Websites visited_NEV_1', 'Probability Correct_NEV_1', 'Manual Correctness Adjustment_NEV_1', 'Non-Expert Validator_NEV_1', 'Feedback_NEV_2', 'Validator Answered Correctly_NEV_2', 'Explanation_NEV_2', 'Self-reported time (minutes)_NEV_2', 'Websites visited_NEV_2', 'Probability Correct_NEV_2', 'Manual Correctness Adjustment_NEV_2', 'Non-Expert Validator_NEV_2', 'Feedback_NEV_3', 'Validator Answered Correctly_NEV_3', 'Explanation_NEV_3', 'Self-reported time (minutes)_NEV_3', 'Websites visited_NEV_3', 'Probability Correct_NEV_3', 'Manual Correctness Adjustment_NEV_3', 'Non-Expert Validator_NEV_3', 'Expert Validator Disagreement Category', 'Canary String']\n",
      "Converting to DataFrame...\n",
      "Saving to ./GPQA/original_data/gpqa_extended.csv...\n",
      "\n",
      "Dataset Info:\n",
      "- Total rows: 546\n",
      "- Columns: ['Pre-Revision Question', 'Pre-Revision Correct Answer', 'Pre-Revision Incorrect Answer 1', 'Pre-Revision Incorrect Answer 2', 'Pre-Revision Incorrect Answer 3', 'Pre-Revision Explanation', 'Self-reported question-writing time (minutes)', 'Question', 'Correct Answer', 'Incorrect Answer 1', 'Incorrect Answer 2', 'Incorrect Answer 3', 'Explanation', 'Revision Comments (from Question Writer)', 'Subdomain', \"Writer's Difficulty Estimate\", 'Extra Revised Question', 'Extra Revised Explanation', 'Extra Revised Correct Answer', 'Extra Revised Incorrect Answer 1', 'Extra Revised Incorrect Answer 2', 'Extra Revised Incorrect Answer 3', 'Non-Expert Validator Accuracy', 'Majority Non-Expert Vals Incorrect', 'Expert Validator Accuracy', 'Record ID', 'High-level domain', 'Question Writer', 'Feedback_EV_1', 'Validator Revision Suggestion_EV_1', 'Is First Validation_EV_1', 'Post hoc agreement_EV_1', 'Sufficient Expertise?_EV_1', 'Understand the question?_EV_1', 'Question Difficulty_EV_1', 'Validator Answered Correctly_EV_1', 'Self-reported time (minutes)_EV_1', 'Probability Correct_EV_1', 'Manual Correctness Adjustment_EV_1', 'Expert Validator_EV_1', 'Feedback_EV_2', 'Validator Revision Suggestion_EV_2', 'Is First Validation_EV_2', 'Post hoc agreement_EV_2', 'Sufficient Expertise?_EV_2', 'Understand the question?_EV_2', 'Question Difficulty_EV_2', 'Validator Answered Correctly_EV_2', 'Self-reported time (minutes)_EV_2', 'Probability Correct_EV_2', 'Manual Correctness Adjustment_EV_2', 'Expert Validator_EV_2', 'Feedback_NEV_1', 'Validator Answered Correctly_NEV_1', 'Explanation_NEV_1', 'Self-reported time (minutes)_NEV_1', 'Websites visited_NEV_1', 'Probability Correct_NEV_1', 'Manual Correctness Adjustment_NEV_1', 'Non-Expert Validator_NEV_1', 'Feedback_NEV_2', 'Validator Answered Correctly_NEV_2', 'Explanation_NEV_2', 'Self-reported time (minutes)_NEV_2', 'Websites visited_NEV_2', 'Probability Correct_NEV_2', 'Manual Correctness Adjustment_NEV_2', 'Non-Expert Validator_NEV_2', 'Feedback_NEV_3', 'Validator Answered Correctly_NEV_3', 'Explanation_NEV_3', 'Self-reported time (minutes)_NEV_3', 'Websites visited_NEV_3', 'Probability Correct_NEV_3', 'Manual Correctness Adjustment_NEV_3', 'Non-Expert Validator_NEV_3', 'Expert Validator Disagreement Category', 'Canary String']\n",
      "- File size: 3.90 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "#wrong format\n",
    "\n",
    "def download_gpqa_diamond_to_csv(output_dir=\"./GPQA/original_data\", filename=\"gpqa_diamond.csv\"):\n",
    "    \"\"\"\n",
    "    Download the GPQA dataset from Hugging Face and save to CSV file.\n",
    "    \n",
    "    Args:\n",
    "        output_dir (str): Directory to save the CSV file\n",
    "        filename (str): Name of the CSV file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Load the dataset from Hugging Face\n",
    "        print(\"Loading GPQA dataset from Hugging Face...\")\n",
    "        dataset = load_dataset(\"Idavidrein/gpqa\", \"gpqa_diamond\", split=\"train\")\n",
    "        \n",
    "        print(f\"Dataset loaded successfully! Found {len(dataset)} examples\")\n",
    "        print(f\"Dataset features: {list(dataset.features.keys())}\")\n",
    "        \n",
    "        # Convert to pandas DataFrame\n",
    "        print(\"Converting to DataFrame...\")\n",
    "        df = dataset.to_pandas()\n",
    "        \n",
    "        # Save to CSV\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        print(f\"Saving to {output_path}...\")\n",
    "        df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "        \n",
    "        # Display some basic info about the dataset\n",
    "        print(\"\\nDataset Info:\")\n",
    "        print(f\"- Total rows: {len(df)}\")\n",
    "        print(f\"- Columns: {list(df.columns)}\")\n",
    "        print(f\"- File size: {os.path.getsize(output_path) / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading dataset: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "def download_gpqa_main_to_csv(output_dir=\"./GPQA/original_data\", filename=\"gpqa_main.csv\"):\n",
    "    \"\"\"\n",
    "    Download the GPQA dataset from Hugging Face and save to CSV file.\n",
    "    \n",
    "    Args:\n",
    "        output_dir (str): Directory to save the CSV file\n",
    "        filename (str): Name of the CSV file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Load the dataset from Hugging Face\n",
    "        print(\"Loading GPQA dataset from Hugging Face...\")\n",
    "        dataset = load_dataset(\"Idavidrein/gpqa\", \"gpqa_main\", split=\"train\")\n",
    "        \n",
    "        print(f\"Dataset loaded successfully! Found {len(dataset)} examples\")\n",
    "        print(f\"Dataset features: {list(dataset.features.keys())}\")\n",
    "        \n",
    "        # Convert to pandas DataFrame\n",
    "        print(\"Converting to DataFrame...\")\n",
    "        df = dataset.to_pandas()\n",
    "        \n",
    "        # Save to CSV\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        print(f\"Saving to {output_path}...\")\n",
    "        df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "        \n",
    "        # Display some basic info about the dataset\n",
    "        print(\"\\nDataset Info:\")\n",
    "        print(f\"- Total rows: {len(df)}\")\n",
    "        print(f\"- Columns: {list(df.columns)}\")\n",
    "        print(f\"- File size: {os.path.getsize(output_path) / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading dataset: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "def download_gpqa_extended_to_csv(output_dir=\"./GPQA/original_data\", filename=\"gpqa_extended.csv\"):\n",
    "    \"\"\"\n",
    "    Download the GPQA dataset from Hugging Face and save to CSV file.\n",
    "    \n",
    "    Args:\n",
    "        output_dir (str): Directory to save the CSV file\n",
    "        filename (str): Name of the CSV file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Load the dataset from Hugging Face\n",
    "        print(\"Loading GPQA dataset from Hugging Face...\")\n",
    "        dataset = load_dataset(\"Idavidrein/gpqa\", \"gpqa_extended\", split=\"train\")\n",
    "        \n",
    "        print(f\"Dataset loaded successfully! Found {len(dataset)} examples\")\n",
    "        print(f\"Dataset features: {list(dataset.features.keys())}\")\n",
    "        \n",
    "        # Convert to pandas DataFrame\n",
    "        print(\"Converting to DataFrame...\")\n",
    "        df = dataset.to_pandas()\n",
    "        \n",
    "        # Save to CSV\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        print(f\"Saving to {output_path}...\")\n",
    "        df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "        \n",
    "        # Display some basic info about the dataset\n",
    "        print(\"\\nDataset Info:\")\n",
    "        print(f\"- Total rows: {len(df)}\")\n",
    "        print(f\"- Columns: {list(df.columns)}\")\n",
    "        print(f\"- File size: {os.path.getsize(output_path) / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading dataset: {e}\")\n",
    "        return None\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    download_gpqa_diamond_to_csv()\n",
    "    download_gpqa_main_to_csv()\n",
    "    download_gpqa_extended_to_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "198it [00:00, 17781.61it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Data preprocess for GPQA\n",
    "Data Link: https://huggingface.co/datasets/Idavidrein/gpqa\n",
    "\"\"\"\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths to data\n",
    "split = 'diamond'  # diamond, main, extended\n",
    "data_path = f'./GPQA/original_data/gpqa_{split}.csv'\n",
    "output_path = f'./GPQA/{split}.json'\n",
    "\n",
    "# Define the keys we want to keep\n",
    "keys_to_keep = [\n",
    "    'id',\n",
    "    'Question',\n",
    "    'Subdomain',\n",
    "    'High-level domain',\n",
    "    'Correct Answer',\n",
    "    'Incorrect Answer 1',\n",
    "    'Incorrect Answer 2',\n",
    "    'Incorrect Answer 3'\n",
    "]\n",
    "\n",
    "filtered_data = []\n",
    "with open(data_path, mode='r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for idx, row in enumerate(tqdm(csv_reader), 0):\n",
    "        # Add id field\n",
    "        row['id'] = idx\n",
    "        # Create new dictionary with only desired keys\n",
    "        filtered_row = {key: row[key] for key in keys_to_keep}\n",
    "\n",
    "        # Extract answers and shuffle them\n",
    "        answers = [\n",
    "            ('Correct Answer', filtered_row['Correct Answer']),\n",
    "            ('Incorrect Answer 1', filtered_row['Incorrect Answer 1']),\n",
    "            ('Incorrect Answer 2', filtered_row['Incorrect Answer 2']),\n",
    "            ('Incorrect Answer 3', filtered_row['Incorrect Answer 3'])\n",
    "        ]\n",
    "        random.shuffle(answers)\n",
    "\n",
    "        # Assign new choices A, B, C, D in order and determine the correct choice\n",
    "        choices = ['A', 'B', 'C', 'D']\n",
    "        formatted_answers = []\n",
    "        correct_choice = None\n",
    "        for i, (label, answer) in enumerate(answers):\n",
    "            choice = choices[i]\n",
    "            formatted_answers.append((choice, answer))\n",
    "            if label == 'Correct Answer':\n",
    "                correct_choice = choice\n",
    "\n",
    "        # Update the Question field\n",
    "        formatted_choices = \"\\n\".join([f\"({choice}) {answer}\" for choice, answer in formatted_answers])\n",
    "        filtered_row['Question'] = f\"{filtered_row['Question']} Choices:\\n{formatted_choices}\\n\"\n",
    "\n",
    "        # Add the Correct Choice field\n",
    "        filtered_row['Correct Choice'] = correct_choice\n",
    "\n",
    "        # Append the updated row to filtered_data\n",
    "        filtered_data.append(filtered_row)\n",
    "\n",
    "# Write the updated data to JSON\n",
    "with open(output_path, mode='w', encoding='utf-8') as json_file:\n",
    "    json.dump(filtered_data, json_file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198/198 [00:00<00:00, 4109.05it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 918.19ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.67s/ shards]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/talzoomanzoo/gpqa_diamond_raw/commit/3ac0c05b7e67fad1df3e16bc3ab312c549999647', commit_message='Upload dataset', commit_description='', oid='3ac0c05b7e67fad1df3e16bc3ab312c549999647', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/talzoomanzoo/gpqa_diamond_raw', endpoint='https://huggingface.co', repo_type='dataset', repo_id='talzoomanzoo/gpqa_diamond_raw'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ABCD\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"./GPQA_DIAMOND\", exist_ok=True)\n",
    "\n",
    "# Load the diamond split\n",
    "test_set = load_dataset(\"Idavidrein/gpqa\", 'gpqa_diamond', split=\"train\")\n",
    "\n",
    "# Processed and formatted dataset\n",
    "filtered_gpqa_diamond_test = []\n",
    "\n",
    "for idx, item in enumerate(tqdm(test_set), 0):\n",
    "    # Clean whitespace from answersls\n",
    "    correct_answer = item['Correct Answer'].strip()\n",
    "    incorrect_answers = [\n",
    "        item['Incorrect Answer 1'].strip(),\n",
    "        item['Incorrect Answer 2'].strip(),\n",
    "        item['Incorrect Answer 3'].strip()\n",
    "    ]\n",
    "\n",
    "    # Create and shuffle answer pool\n",
    "    answers = {'Correct Answer': correct_answer}\n",
    "    answers.update({f'Incorrect Answer {i+1}': ans for i, ans in enumerate(incorrect_answers)})\n",
    "    answers = list(answers.values())\n",
    "\n",
    "    # Format choices\n",
    "    # choice_labels = ['A', 'B', 'C', 'D']\n",
    "    choice_list = []\n",
    "    correct_choice = None\n",
    "\n",
    "    for i, answer in enumerate(answers):\n",
    "        choice_list.append(answer)\n",
    "        if answer == correct_answer:\n",
    "            correct_choice = answer\n",
    "\n",
    "    # Format the full question\n",
    "    # formatted_question = f\"{item['Question'].strip()} Choices:\\n\" + \"\\n\".join(formatted_choices) + \"\\n\"\n",
    "    random.seed(42)\n",
    "    choice_list = random.shuffle(choice_list)\n",
    "    # Add to final dataset\n",
    "    filtered_gpqa_diamond_test.append({\n",
    "        'id': idx,\n",
    "        'Subdomain': item['Subdomain'],\n",
    "        'High-level domain': item['High-level domain'],\n",
    "        'Question': item['Question'],\n",
    "        'Choices': choice_list,\n",
    "        'Correct Choice': item['Correct Answer']\n",
    "    })\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"./GPQA_DIAMOND/test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_gpqa_diamond_test, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Push to Hugging Face Hub\n",
    "dataset = Dataset.from_list(filtered_gpqa_diamond_test)\n",
    "dataset.push_to_hub('talzoomanzoo/gpqa_diamond_raw', private=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198/198 [00:00<00:00, 4134.83it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 985.74ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.05 shards/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/talzoomanzoo/gpqa_diamond/commit/ff55adbeecdcc6f8697c6a4a3212b96785ac47c9', commit_message='Upload dataset', commit_description='', oid='ff55adbeecdcc6f8697c6a4a3212b96785ac47c9', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/talzoomanzoo/gpqa_diamond', endpoint='https://huggingface.co', repo_type='dataset', repo_id='talzoomanzoo/gpqa_diamond'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ABCD\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"./GPQA_DIAMOND\", exist_ok=True)\n",
    "\n",
    "# Load the diamond split\n",
    "test_set = load_dataset(\"Idavidrein/gpqa\", 'gpqa_diamond', split=\"train\")\n",
    "\n",
    "# Processed and formatted dataset\n",
    "filtered_gpqa_diamond_test = []\n",
    "\n",
    "for idx, item in enumerate(tqdm(test_set), 0):\n",
    "    # Clean whitespace from answers\n",
    "    correct_answer = item['Correct Answer'].strip()\n",
    "    incorrect_answers = [\n",
    "        item['Incorrect Answer 1'].strip(),\n",
    "        item['Incorrect Answer 2'].strip(),\n",
    "        item['Incorrect Answer 3'].strip()\n",
    "    ]\n",
    "\n",
    "    # Create and shuffle answer pool\n",
    "    answers = [('Correct Answer', correct_answer)]\n",
    "    answers += [(f'Incorrect Answer {i+1}', ans) for i, ans in enumerate(incorrect_answers)]\n",
    "    random.seed(42)\n",
    "    random.shuffle(answers)\n",
    "\n",
    "    # Assign A–D and format choices\n",
    "    choice_labels = ['A', 'B', 'C', 'D']\n",
    "    choice_dict = {}\n",
    "    formatted_choices = []\n",
    "    correct_choice = None\n",
    "\n",
    "    for i, (label, answer) in enumerate(answers):\n",
    "        choice_letter = choice_labels[i]\n",
    "        choice_dict[choice_letter] = answer\n",
    "        formatted_choices.append(f\"({choice_letter}) {answer}\")\n",
    "        if label == 'Correct Answer':\n",
    "            correct_choice = choice_letter\n",
    "\n",
    "    # Format the full question\n",
    "    formatted_question = f\"{item['Question'].strip()} Choices:\\n\" + \"\\n\".join(formatted_choices) + \"\\n\"\n",
    "\n",
    "    # Add to final dataset\n",
    "    filtered_gpqa_diamond_test.append({\n",
    "        'id': idx,\n",
    "        'Subdomain': item['Subdomain'],\n",
    "        'High-level domain': item['High-level domain'],\n",
    "        'Question': formatted_question,\n",
    "        'Choices': choice_dict,\n",
    "        'Correct Choice': correct_choice\n",
    "    })\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"./GPQA_DIAMOND/test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_gpqa_diamond_test, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Push to Hugging Face Hub\n",
    "dataset = Dataset.from_list(filtered_gpqa_diamond_test)\n",
    "dataset.push_to_hub('talzoomanzoo/gpqa_diamond', private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [00:00<00:00, 4173.54it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 509.70ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.26 shards/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/talzoomanzoo/gpqa_extended/commit/b8d3e9017ed56da265461393399103b0ade18ffb', commit_message='Upload dataset', commit_description='', oid='b8d3e9017ed56da265461393399103b0ade18ffb', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/talzoomanzoo/gpqa_extended', endpoint='https://huggingface.co', repo_type='dataset', repo_id='talzoomanzoo/gpqa_extended'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ABCD\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "temp_cache_dir = \"/home/mjgwak/.cache/tmp_hf_cache\"\n",
    "os.makedirs(temp_cache_dir, exist_ok=True)\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"./GPQA_EXTENDED\", exist_ok=True)\n",
    "\n",
    "# Load the diamond split\n",
    "test_set = load_dataset(\"Idavidrein/gpqa\", \"gpqa_extended\", split=\"train\", cache_dir=temp_cache_dir)\n",
    "\n",
    "# Processed and formatted dataset\n",
    "filtered_gpqa_extended_test = []\n",
    "\n",
    "for idx, item in enumerate(tqdm(test_set), 0):\n",
    "    # Clean whitespace from answers\n",
    "    correct_answer = item['Correct Answer'].strip()\n",
    "    incorrect_answers = [\n",
    "        item['Incorrect Answer 1'].strip(),\n",
    "        item['Incorrect Answer 2'].strip(),\n",
    "        item['Incorrect Answer 3'].strip()\n",
    "    ]\n",
    "\n",
    "    # Create and shuffle answer pool\n",
    "    answers = [('Correct Answer', correct_answer)]\n",
    "    answers += [(f'Incorrect Answer {i+1}', ans) for i, ans in enumerate(incorrect_answers)]\n",
    "    random.seed(42)\n",
    "    random.shuffle(answers)\n",
    "\n",
    "    # Assign A–D and format choices\n",
    "    choice_labels = ['A', 'B', 'C', 'D']\n",
    "    choice_dict = {}\n",
    "    formatted_choices = []\n",
    "    correct_choice = None\n",
    "\n",
    "    for i, (label, answer) in enumerate(answers):\n",
    "        choice_letter = choice_labels[i]\n",
    "        choice_dict[choice_letter] = answer\n",
    "        formatted_choices.append(f\"({choice_letter}) {answer}\")\n",
    "        if label == 'Correct Answer':\n",
    "            correct_choice = choice_letter\n",
    "\n",
    "    # Format the full question\n",
    "    formatted_question = f\"{item['Question'].strip()} Choices:\\n\" + \"\\n\".join(formatted_choices) + \"\\n\"\n",
    "\n",
    "    # Add to final dataset\n",
    "    filtered_gpqa_extended_test.append({\n",
    "        'id': idx,\n",
    "        'Subdomain': item['Subdomain'],\n",
    "        'High-level domain': item['High-level domain'],\n",
    "        'Question': formatted_question,\n",
    "        'Choices': choice_dict,\n",
    "        'Correct Choice': correct_choice\n",
    "    })\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"./GPQA_EXTENDED/test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_gpqa_extended_test, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Push to Hugging Face Hub\n",
    "dataset = Dataset.from_list(filtered_gpqa_extended_test)\n",
    "dataset.push_to_hub('talzoomanzoo/gpqa_extended', private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 448/448 [00:00<00:00, 4168.71it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 209.58ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.06s/ shards]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/talzoomanzoo/gpqa_main/commit/514362349ce5a4768658a01d58479e953b89d2ec', commit_message='Upload dataset', commit_description='', oid='514362349ce5a4768658a01d58479e953b89d2ec', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/talzoomanzoo/gpqa_main', endpoint='https://huggingface.co', repo_type='dataset', repo_id='talzoomanzoo/gpqa_main'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ABCD\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "temp_cache_dir = \"/home/mjgwak/.cache/tmp_hf_cache\"\n",
    "os.makedirs(temp_cache_dir, exist_ok=True)\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"./GPQA_MAIN\", exist_ok=True)\n",
    "\n",
    "# Load the diamond split\n",
    "test_set = load_dataset(\"Idavidrein/gpqa\", \"gpqa_main\", split=\"train\", cache_dir=temp_cache_dir)\n",
    "\n",
    "# Processed and formatted dataset\n",
    "filtered_gpqa_main_test = []\n",
    "\n",
    "for idx, item in enumerate(tqdm(test_set), 0):\n",
    "    # Clean whitespace from answers\n",
    "    correct_answer = item['Correct Answer'].strip()\n",
    "    incorrect_answers = [\n",
    "        item['Incorrect Answer 1'].strip(),\n",
    "        item['Incorrect Answer 2'].strip(),\n",
    "        item['Incorrect Answer 3'].strip()\n",
    "    ]\n",
    "\n",
    "    # Create and shuffle answer pool\n",
    "    answers = [('Correct Answer', correct_answer)]\n",
    "    answers += [(f'Incorrect Answer {i+1}', ans) for i, ans in enumerate(incorrect_answers)]\n",
    "    random.seed(42)\n",
    "    random.shuffle(answers)\n",
    "\n",
    "    # Assign A–D and format choices\n",
    "    choice_labels = ['A', 'B', 'C', 'D']\n",
    "    choice_dict = {}\n",
    "    formatted_choices = []\n",
    "    correct_choice = None\n",
    "\n",
    "    for i, (label, answer) in enumerate(answers):\n",
    "        choice_letter = choice_labels[i]\n",
    "        choice_dict[choice_letter] = answer\n",
    "        formatted_choices.append(f\"({choice_letter}) {answer}\")\n",
    "        if label == 'Correct Answer':\n",
    "            correct_choice = choice_letter\n",
    "\n",
    "    # Format the full question\n",
    "    formatted_question = f\"{item['Question'].strip()} Choices:\\n\" + \"\\n\".join(formatted_choices) + \"\\n\"\n",
    "\n",
    "    # Add to final dataset\n",
    "    filtered_gpqa_main_test.append({\n",
    "        'id': idx,\n",
    "        'Subdomain': item['Subdomain'],\n",
    "        'High-level domain': item['High-level domain'],\n",
    "        'Question': formatted_question,\n",
    "        'Choices': choice_dict,\n",
    "        'Correct Choice': correct_choice\n",
    "    })\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"./GPQA_MAIN/test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_gpqa_main_test, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Push to Hugging Face Hub\n",
    "dataset = Dataset.from_list(filtered_gpqa_main_test)\n",
    "dataset.push_to_hub('talzoomanzoo/gpqa_main', private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 769.03ba/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.53s/it]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.19s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 862.67ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.02it/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.62s/ shards]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/talzoomanzoo/gsm8k/commit/46501e87b15df04e7d42dd934f065be523c8a13c', commit_message='Upload dataset', commit_description='', oid='46501e87b15df04e7d42dd934f065be523c8a13c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/talzoomanzoo/gsm8k', endpoint='https://huggingface.co', repo_type='dataset', repo_id='talzoomanzoo/gsm8k'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gsm8k\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"./GSM8K\"):\n",
    "    os.makedirs(\"./GSM8K\")\n",
    "\n",
    "train_set = load_dataset(\"skrishna/gsm8k_only_answer\", split=\"train\")\n",
    "test_set = load_dataset(\"skrishna/gsm8k_only_answer\", split=\"test\")\n",
    "\n",
    "# Prepare train data\n",
    "gsm8k_train = []\n",
    "for item in train_set:\n",
    "    gsm8k_train.append({\n",
    "        'Question': item['text'],\n",
    "        'answer': item['label'],\n",
    "    })\n",
    "\n",
    "# Prepare test data\n",
    "gsm8k_test = []\n",
    "for item in test_set: #1319\n",
    "    gsm8k_test.append({\n",
    "        'Question': item['text'],\n",
    "        'answer': item['label'],\n",
    "    })\n",
    "\n",
    "# Save individual files\n",
    "with open(\"./GSM8K/train.json\", \"w\") as f:\n",
    "    json.dump(gsm8k_train, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(\"./GSM8K/test.json\", \"w\") as f:\n",
    "    json.dump(gsm8k_test, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Create DatasetDict with both splits\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': Dataset.from_list(gsm8k_train),\n",
    "    'test': Dataset.from_list(gsm8k_test)\n",
    "})\n",
    "\n",
    "# Upload to hub with both splits\n",
    "dataset_dict.push_to_hub('talzoomanzoo/gsm8k', private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"./AIME\"):\n",
    "    os.makedirs(\"./AIME\")\n",
    "\n",
    "train_set = load_dataset(\"gneubig/aime-1983-2024\", split=\"train\")\n",
    "test_set1 = load_dataset(\"opencompass/AIME2025\", 'AIME2025-I', split=\"test\")\n",
    "test_set2 = load_dataset(\"opencompass/AIME2025\", 'AIME2025-II', split=\"test\")\n",
    "with open(\"./AIME/train.json\", \"w\") as f:\n",
    "    aime_train = []\n",
    "    for item in train_set:\n",
    "        aime_train.append({\n",
    "            'id': item['ID'],\n",
    "            'year': item['Year'],\n",
    "            'problem_number': item['Problem Number'],\n",
    "            'Question': item['Question'],\n",
    "            'answer': item['Answer'],\n",
    "        })\n",
    "    json.dump(aime_train, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(\"./AIME/test.json\", \"w\") as f:\n",
    "    aime_test = []\n",
    "    index = 0\n",
    "    for item in test_set1:\n",
    "        aime_test.append({\n",
    "            'id': index,\n",
    "            'Question': item['question'],\n",
    "            'answer': item['answer'],\n",
    "        })\n",
    "        index += 1\n",
    "    for item in test_set2:\n",
    "        aime_test.append({\n",
    "            'id': index,\n",
    "            'Question': item['question'],\n",
    "            'answer': item['answer'],\n",
    "        })\n",
    "        index += 1\n",
    "    json.dump(aime_test, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 2686.93ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.77it/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/talzoomanzoo/aime_test/commit/c70089cb8b81a4411783e9e54bc2831686e92773', commit_message='Upload dataset', commit_description='', oid='c70089cb8b81a4411783e9e54bc2831686e92773', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/talzoomanzoo/aime_test', endpoint='https://huggingface.co', repo_type='dataset', repo_id='talzoomanzoo/aime_test'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"./AIME\"):\n",
    "    os.makedirs(\"./AIME\")\n",
    "\n",
    "test_set1 = load_dataset(\"opencompass/AIME2025\", 'AIME2025-I', split=\"test\")\n",
    "test_set2 = load_dataset(\"opencompass/AIME2025\", 'AIME2025-II', split=\"test\")\n",
    "\n",
    "with open(\"./AIME/test.json\", \"w\") as f:\n",
    "    aime_test = []\n",
    "    for item in test_set1:\n",
    "        aime_test.append({\n",
    "            'Question': item['question'],\n",
    "            'answer': item['answer'],\n",
    "        })\n",
    "    for item in test_set2:\n",
    "        aime_test.append({\n",
    "            'Question': item['question'],\n",
    "            'answer': item['answer'],\n",
    "        })\n",
    "\n",
    "    print(len(aime_test))\n",
    "    json.dump(aime_test, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "dataset = Dataset.from_list(aime_test)\n",
    "dataset.push_to_hub('talzoomanzoo/aime_test', private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"./simplescaling\"):\n",
    "    os.makedirs(\"./simplescaling\")\n",
    "\n",
    "train_set = load_dataset(\"simplescaling/s1K_tokenized\", split=\"train\")\n",
    "\n",
    "with open(\"./simplescaling/train.json\", \"w\") as f:\n",
    "    simplescaling_train = []\n",
    "    for item in train_set:\n",
    "        # Create new dictionary with only the desired fields\n",
    "        filtered_item = {\n",
    "            'Question': item['question'],\n",
    "            'solution': item['solution'],\n",
    "            'source_type': item['source_type']\n",
    "        }\n",
    "        simplescaling_train.append(filtered_item)\n",
    "    json.dump(simplescaling_train, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/datasets/LangAGI-Lab/medbullets/resolve/5fdb3d4671ea868857b2422f69c7b0f2fc8b4349/MedBullets.py",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/datasets/load.py:1580\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1580\u001b[0m     dataset_script_path \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _require_custom_configs \u001b[38;5;129;01mor\u001b[39;00m (revision \u001b[38;5;129;01mand\u001b[39;00m revision \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/huggingface_hub/hf_api.py:5475\u001b[0m, in \u001b[0;36mHfApi.hf_hub_download\u001b[0;34m(self, repo_id, filename, subfolder, repo_type, revision, cache_dir, local_dir, force_download, proxies, etag_timeout, token, local_files_only, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   5473\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken\n\u001b[0;32m-> 5475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5478\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5480\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5487\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5491\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5496\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/huggingface_hub/file_download.py:961\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/huggingface_hub/file_download.py:1024\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[0;32m-> 1024\u001b[0m (url_to_download, etag, commit_hash, expected_size, xet_file_data, head_call_error) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/huggingface_hub/file_download.py:1484\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1484\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/huggingface_hub/file_download.py:1401\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1401\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/huggingface_hub/file_download.py:304\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    303\u001b[0m         next_url \u001b[38;5;241m=\u001b[39m urlparse(url)\u001b[38;5;241m.\u001b[39m_replace(path\u001b[38;5;241m=\u001b[39mparsed_target\u001b[38;5;241m.\u001b[39mpath)\u001b[38;5;241m.\u001b[39mgeturl()\n\u001b[0;32m--> 304\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnext_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/huggingface_hub/file_download.py:285\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 285\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/huggingface_hub/file_download.py:309\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    308\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 309\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:420\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    419\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntry Not Found for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(EntryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGatedRepo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-680dcc83-698869701284e4596335ca75;f783c99a-3e43-4b84-b9c5-3f3627c77a64)\n\nEntry Not Found for url: https://huggingface.co/datasets/LangAGI-Lab/medbullets/resolve/5fdb3d4671ea868857b2422f69c7b0f2fc8b4349/MedBullets.py.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      7\u001b[0m train_set \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLangAGI-Lab/MedBullets\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m test_set \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLangAGI-Lab/MedBullets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./medical/medbullets_train.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     11\u001b[0m     medbullets_train \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/datasets/load.py:2062\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2057\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2058\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2059\u001b[0m )\n\u001b[1;32m   2061\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2062\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2073\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2074\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2075\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2080\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/datasets/load.py:1782\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1781\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1782\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1795\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/datasets/load.py:1629\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1619\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1620\u001b[0m         use_exported_dataset_infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHubDatasetModuleFactoryWithoutScript\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_exported_dataset_infos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_exported_dataset_infos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1631\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is a gated dataset on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/datasets/load.py:1019\u001b[0m, in \u001b[0;36mHubDatasetModuleFactoryWithoutScript.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1018\u001b[0m     patterns \u001b[38;5;241m=\u001b[39m get_data_patterns(base_path, download_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_config)\n\u001b[0;32m-> 1019\u001b[0m data_files \u001b[38;5;241m=\u001b[39m \u001b[43mDataFilesDict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALL_ALLOWED_EXTENSIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m module_name, default_builder_kwargs \u001b[38;5;241m=\u001b[39m infer_module_for_data_files(\n\u001b[1;32m   1026\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[1;32m   1027\u001b[0m     path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m   1028\u001b[0m     download_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_config,\n\u001b[1;32m   1029\u001b[0m )\n\u001b[1;32m   1030\u001b[0m data_files \u001b[38;5;241m=\u001b[39m data_files\u001b[38;5;241m.\u001b[39mfilter(\n\u001b[1;32m   1031\u001b[0m     extensions\u001b[38;5;241m=\u001b[39m_MODULE_TO_EXTENSIONS[module_name], file_names\u001b[38;5;241m=\u001b[39m_MODULE_TO_METADATA_FILE_NAMES[module_name]\n\u001b[1;32m   1032\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/datasets/data_files.py:690\u001b[0m, in \u001b[0;36mDataFilesDict.from_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    685\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m()\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, patterns_for_key \u001b[38;5;129;01min\u001b[39;00m patterns\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    687\u001b[0m     out[key] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    688\u001b[0m         patterns_for_key\n\u001b[1;32m    689\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(patterns_for_key, DataFilesList)\n\u001b[0;32m--> 690\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mDataFilesList\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatterns_for_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    696\u001b[0m     )\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/datasets/data_files.py:583\u001b[0m, in \u001b[0;36mDataFilesList.from_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m patterns:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    582\u001b[0m         data_files\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m--> 583\u001b[0m             \u001b[43mresolve_pattern\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m                \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m         )\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m    591\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_magic(pattern):\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/datasets/data_files.py:361\u001b[0m, in \u001b[0;36mresolve_pattern\u001b[0;34m(pattern, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m config\u001b[38;5;241m.\u001b[39mHF_HUB_VERSION \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.20.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# 10 times faster glob with detail=True (ignores costly info like lastCommit)\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     glob_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpand_info\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    359\u001b[0m matched_paths \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    360\u001b[0m     filepath \u001b[38;5;28;01mif\u001b[39;00m filepath\u001b[38;5;241m.\u001b[39mstartswith(protocol_prefix) \u001b[38;5;28;01melse\u001b[39;00m protocol_prefix \u001b[38;5;241m+\u001b[39m filepath\n\u001b[0;32m--> 361\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filepath, info \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetail\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mglob_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mislink\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(filepath))))\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (xbasename(filepath) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m files_to_ignore)\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_inside_unrequested_special_dir(filepath, fs_pattern)\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir(filepath, fs_pattern)\n\u001b[1;32m    366\u001b[0m ]  \u001b[38;5;66;03m# ignore .ipynb and __pycache__, but keep /../\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allowed_extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    368\u001b[0m     out \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    369\u001b[0m         filepath\n\u001b[1;32m    370\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m filepath \u001b[38;5;129;01min\u001b[39;00m matched_paths\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m suffix \u001b[38;5;129;01min\u001b[39;00m allowed_extensions \u001b[38;5;28;01mfor\u001b[39;00m suffix \u001b[38;5;129;01min\u001b[39;00m xbasename(filepath)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m    372\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py:521\u001b[0m, in \u001b[0;36mHfFileSystem.glob\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpand_info\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetail\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m    520\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_path(path, revision\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39munresolve()\n\u001b[0;32m--> 521\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/fsspec/spec.py:609\u001b[0m, in \u001b[0;36mAbstractFileSystem.glob\u001b[0;34m(self, path, maxdepth, **kwargs)\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    607\u001b[0m         depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 609\u001b[0m allpaths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwithdirs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetail\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    611\u001b[0m pattern \u001b[38;5;241m=\u001b[39m glob_translate(path \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ends_with_sep \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    612\u001b[0m pattern \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(pattern)\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py:556\u001b[0m, in \u001b[0;36mHfFileSystem.find\u001b[0;34m(self, path, maxdepth, withdirs, detail, refresh, revision, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;124;03mList all files below path.\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;124;03m    `Union[List[str], Dict[str, Dict[str, Any]]]`: List of paths or dict of file information.\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxdepth:\n\u001b[0;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwithdirs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwithdirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetail\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetail\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefresh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m resolved_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_path(path, revision\u001b[38;5;241m=\u001b[39mrevision)\n\u001b[1;32m    560\u001b[0m path \u001b[38;5;241m=\u001b[39m resolved_path\u001b[38;5;241m.\u001b[39munresolve()\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/fsspec/spec.py:500\u001b[0m, in \u001b[0;36mAbstractFileSystem.find\u001b[0;34m(self, path, maxdepth, withdirs, detail, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# Add the root directory if withdirs is requested\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# This is needed for posix glob compliance\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m withdirs \u001b[38;5;129;01mand\u001b[39;00m path \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misdir(path):\n\u001b[0;32m--> 500\u001b[0m     out[path] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, dirs, files \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwalk(path, maxdepth, detail\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m withdirs:\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py:719\u001b[0m, in \u001b[0;36mHfFileSystem.info\u001b[0;34m(self, path, refresh, revision, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m     out \u001b[38;5;241m=\u001b[39m out1[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m refresh \u001b[38;5;129;01mor\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (expand_info \u001b[38;5;129;01mand\u001b[39;00m out \u001b[38;5;129;01mand\u001b[39;00m out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_commit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 719\u001b[0m     paths_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_paths_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_in_repo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolved_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolved_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m paths_info:\n\u001b[1;32m    727\u001b[0m         _raise_file_not_found(path, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/huggingface_hub/hf_api.py:3354\u001b[0m, in \u001b[0;36mHfApi.get_paths_info\u001b[0;34m(self, repo_id, paths, expand, revision, repo_type, token)\u001b[0m\n\u001b[1;32m   3351\u001b[0m revision \u001b[38;5;241m=\u001b[39m quote(revision, safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m constants\u001b[38;5;241m.\u001b[39mDEFAULT_REVISION\n\u001b[1;32m   3352\u001b[0m headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_hf_headers(token\u001b[38;5;241m=\u001b[39mtoken)\n\u001b[0;32m-> 3354\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3355\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/api/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrepo_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43ms/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrepo_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/paths-info/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrevision\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m   3357\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpaths\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3358\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexpand\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3359\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3362\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m   3363\u001b[0m paths_info \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/requests/sessions.py:637\u001b[0m, in \u001b[0;36mSession.post\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    627\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:96\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/reasoning/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"Data preprocess for MedBullets\n",
    "Data Link: https://huggingface.co/datasets/LangAGI-Lab/MedBullets\n",
    "\"\"\"\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "train_set = load_dataset(\"LangAGI-Lab/MedBullets\", split=\"train\")\n",
    "test_set = load_dataset(\"LangAGI-Lab/MedBullets\", split=\"test\")\n",
    "\n",
    "with open (\"./medical/medbullets_train.json\", \"w\") as f:\n",
    "    medbullets_train = []\n",
    "    for item in train_set:\n",
    "        item['Question'] = item.pop('question', None)\n",
    "        medbullets_train.append(item)\n",
    "    json.dump(medbullets_train, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open (\"./medical/medbullets_test.json\", \"w\") as f:\n",
    "    medbullets_test = []\n",
    "    for item in test_set:\n",
    "        item['Question'] = item.pop('question', None)\n",
    "        medbullets_test.append(item)\n",
    "    json.dump(medbullets_test, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data preprocess for JAMA\n",
    "Data Link: https://huggingface.co/datasets/LangAGI-Lab/jama_full\n",
    "\"\"\"\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "train_set = load_dataset(\"LangAGI-Lab/jama_full\", split=\"train\")\n",
    "test_set = load_dataset(\"LangAGI-Lab/jama_full\", split=\"test\")\n",
    "\n",
    "with open (\"./medical/jama_full_train.json\", \"w\") as f:\n",
    "    jama_full_train = []\n",
    "    for item in train_set:\n",
    "        item['Question'] = item.pop('question', None)\n",
    "        jama_full_train.append(item)\n",
    "    json.dump(jama_full_train, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open (\"./medical/jama_full_test.json\", \"w\") as f:\n",
    "    jama_full_test = []\n",
    "    for item in test_set:\n",
    "        item['Question'] = item.pop('question', None)\n",
    "        jama_full_test.append(item)\n",
    "    json.dump(jama_full_test, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data preprocess for GPQA\n",
    "Data Link: https://huggingface.co/datasets/Idavidrein/gpqa\n",
    "\"\"\"\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths to data\n",
    "split = 'diamond'  # diamond, main, extended\n",
    "data_path = f'./GPQA/original_data/gpqa_{split}.csv'\n",
    "output_path = f'./GPQA/{split}.json'\n",
    "\n",
    "# Define the keys we want to keep\n",
    "keys_to_keep = [\n",
    "    'id',\n",
    "    'Question',\n",
    "    'Subdomain',\n",
    "    'High-level domain',\n",
    "    'Correct Answer',\n",
    "    'Incorrect Answer 1',\n",
    "    'Incorrect Answer 2',\n",
    "    'Incorrect Answer 3'\n",
    "]\n",
    "\n",
    "filtered_data = []\n",
    "with open(data_path, mode='r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for idx, row in enumerate(tqdm(csv_reader), 0):\n",
    "        # Add id field\n",
    "        row['id'] = idx\n",
    "        # Create new dictionary with only desired keys\n",
    "        filtered_row = {key: row[key] for key in keys_to_keep}\n",
    "\n",
    "        # Extract answers and shuffle them\n",
    "        answers = [\n",
    "            ('Correct Answer', filtered_row['Correct Answer']),\n",
    "            ('Incorrect Answer 1', filtered_row['Incorrect Answer 1']),\n",
    "            ('Incorrect Answer 2', filtered_row['Incorrect Answer 2']),\n",
    "            ('Incorrect Answer 3', filtered_row['Incorrect Answer 3'])\n",
    "        ]\n",
    "        random.shuffle(answers)\n",
    "\n",
    "        # Assign new choices A, B, C, D in order and determine the correct choice\n",
    "        choices = ['A', 'B', 'C', 'D']\n",
    "        formatted_answers = []\n",
    "        correct_choice = None\n",
    "        for i, (label, answer) in enumerate(answers):\n",
    "            choice = choices[i]\n",
    "            formatted_answers.append((choice, answer))\n",
    "            if label == 'Correct Answer':\n",
    "                correct_choice = choice\n",
    "\n",
    "        # Update the Question field\n",
    "        formatted_choices = \"\\n\".join([f\"({choice}) {answer}\" for choice, answer in formatted_answers])\n",
    "        filtered_row['Question'] = f\"{filtered_row['Question']} Choices:\\n{formatted_choices}\\n\"\n",
    "\n",
    "        # Add the Correct Choice field\n",
    "        filtered_row['Correct Choice'] = correct_choice\n",
    "\n",
    "        # Append the updated row to filtered_data\n",
    "        filtered_data.append(filtered_row)\n",
    "\n",
    "# Write the updated data to JSON\n",
    "with open(output_path, mode='w', encoding='utf-8') as json_file:\n",
    "    json.dump(filtered_data, json_file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown split \"train\". Should be one of ['test'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./MATH500\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m test_set \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuggingFaceH4/MATH-500\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m train_set \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHuggingFaceH4/MATH-500\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./MATH500/test.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     17\u001b[0m     math500_test \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/load.py:2096\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2093\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2094\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2095\u001b[0m )\n\u001b[0;32m-> 2096\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_infos:\n\u001b[1;32m   2098\u001b[0m     builder_instance\u001b[38;5;241m.\u001b[39m_save_infos()\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/builder.py:1127\u001b[0m, in \u001b[0;36mDatasetBuilder.as_dataset\u001b[0;34m(self, split, run_post_process, verification_mode, in_memory)\u001b[0m\n\u001b[1;32m   1124\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS)\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# Create a dataset for each of the given splits\u001b[39;00m\n\u001b[0;32m-> 1127\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_single_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_post_process\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_post_process\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(datasets, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   1139\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m DatasetDict(datasets)\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/utils/py_utils.py:494\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    493\u001b[0m     data_struct \u001b[38;5;241m=\u001b[39m [data_struct]\n\u001b[0;32m--> 494\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    496\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m mapped[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/builder.py:1157\u001b[0m, in \u001b[0;36mDatasetBuilder._build_single_dataset\u001b[0;34m(self, split, run_post_process, verification_mode, in_memory)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     split \u001b[38;5;241m=\u001b[39m Split(split)\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;66;03m# Build base dataset\u001b[39;00m\n\u001b[0;32m-> 1157\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_as_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_post_process:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m resource_file_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_processing_resources(split)\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/builder.py:1231\u001b[0m, in \u001b[0;36mDatasetBuilder._as_dataset\u001b[0;34m(self, split, in_memory)\u001b[0m\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_legacy_cache():\n\u001b[1;32m   1230\u001b[0m     dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m-> 1231\u001b[0m dataset_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mArrowReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstructions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1237\u001b[0m fingerprint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_dataset_fingerprint(split)\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Dataset(fingerprint\u001b[38;5;241m=\u001b[39mfingerprint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/arrow_reader.py:248\u001b[0m, in \u001b[0;36mBaseReader.read\u001b[0;34m(self, name, instructions, split_infos, in_memory)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread\u001b[39m(\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    229\u001b[0m     name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m     in_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    233\u001b[0m ):\n\u001b[1;32m    234\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns Dataset instance(s).\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m         kwargs to build a single Dataset instance.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m     files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_file_instructions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m files:\n\u001b[1;32m    250\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstruction \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstructions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m corresponds to no data!\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/arrow_reader.py:221\u001b[0m, in \u001b[0;36mBaseReader.get_file_instructions\u001b[0;34m(self, name, instruction, split_infos)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_file_instructions\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, instruction, split_infos):\n\u001b[1;32m    220\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return list of dict {'filename': str, 'skip': int, 'take': int}\"\"\"\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m     file_instructions \u001b[38;5;241m=\u001b[39m \u001b[43mmake_file_instructions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiletype_suffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filetype_suffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_path\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m     files \u001b[38;5;241m=\u001b[39m file_instructions\u001b[38;5;241m.\u001b[39mfile_instructions\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m files\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/arrow_reader.py:130\u001b[0m, in \u001b[0;36mmake_file_instructions\u001b[0;34m(name, split_infos, instruction, filetype_suffix, prefix_path)\u001b[0m\n\u001b[1;32m    128\u001b[0m     instruction \u001b[38;5;241m=\u001b[39m ReadInstruction\u001b[38;5;241m.\u001b[39mfrom_spec(instruction)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Create the absolute instruction (per split)\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m absolute_instructions \u001b[38;5;241m=\u001b[39m \u001b[43minstruction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_absolute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname2len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# For each split, return the files instruction (skip/take)\u001b[39;00m\n\u001b[1;32m    133\u001b[0m file_instructions \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/arrow_reader.py:620\u001b[0m, in \u001b[0;36mReadInstruction.to_absolute\u001b[0;34m(self, name2len)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mto_absolute\u001b[39m(\u001b[38;5;28mself\u001b[39m, name2len):\n\u001b[1;32m    609\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Translate instruction into a list of absolute instructions.\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \n\u001b[1;32m    611\u001b[0m \u001b[38;5;124;03m    Those absolute instructions are then to be added together.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;124;03m        list of _AbsoluteInstruction instances (corresponds to the + in spec).\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [_rel_to_abs_instr(rel_instr, name2len) \u001b[38;5;28;01mfor\u001b[39;00m rel_instr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_relative_instructions]\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/arrow_reader.py:620\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mto_absolute\u001b[39m(\u001b[38;5;28mself\u001b[39m, name2len):\n\u001b[1;32m    609\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Translate instruction into a list of absolute instructions.\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \n\u001b[1;32m    611\u001b[0m \u001b[38;5;124;03m    Those absolute instructions are then to be added together.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;124;03m        list of _AbsoluteInstruction instances (corresponds to the + in spec).\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43m_rel_to_abs_instr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrel_instr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname2len\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m rel_instr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_relative_instructions]\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/arrow_reader.py:437\u001b[0m, in \u001b[0;36m_rel_to_abs_instr\u001b[0;34m(rel_instr, name2len)\u001b[0m\n\u001b[1;32m    435\u001b[0m split \u001b[38;5;241m=\u001b[39m rel_instr\u001b[38;5;241m.\u001b[39msplitname\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m name2len:\n\u001b[0;32m--> 437\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown split \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(name2len)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    438\u001b[0m num_examples \u001b[38;5;241m=\u001b[39m name2len[split]\n\u001b[1;32m    439\u001b[0m from_ \u001b[38;5;241m=\u001b[39m rel_instr\u001b[38;5;241m.\u001b[39mfrom_\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown split \"train\". Should be one of ['test']."
     ]
    }
   ],
   "source": [
    "\"\"\"Data preprocess for MATH500\n",
    "Data Link: https://huggingface.co/datasets/HuggingFaceH4/MATH-500\n",
    "\"\"\"\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "if not os.path.exists(\"./MATH500\"):\n",
    "    os.makedirs(\"./MATH500\")\n",
    "\n",
    "test_set = load_dataset(\"HuggingFaceH4/MATH-500\", split=\"test\")\n",
    "\n",
    "with open(\"./MATH500/test.json\", 'w') as file:\n",
    "    math500_test = []\n",
    "    for id, line in enumerate(tqdm(test_set)):\n",
    "        math500_test.append({\n",
    "            'id': id, \n",
    "            'Question': line['problem'],\n",
    "            'solution': line['solution'],\n",
    "            'answer': line['answer'],\n",
    "            'subject': line['subject'],\n",
    "            'level': line['level'],\n",
    "            'unique_id': line['unique_id'],\n",
    "        })\n",
    "\n",
    "# Write the updated data to JSON\n",
    "with open(\"./MATH500/test.json\", mode='w', encoding='utf-8') as json_file:\n",
    "    json.dump(math500_test, json_file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:00<00:00, 43015.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Data preprocess for AMC\n",
    "\"\"\"\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "if not os.path.exists(\"./AMC\"):\n",
    "    os.makedirs(\"./AMC\")\n",
    "\n",
    "test_set = load_dataset(\"AI-MO/aimo-validation-amc\", split=\"train\")\n",
    "\n",
    "data_list = []\n",
    "with open(\"./AMC/test.json\", 'w') as file:\n",
    "    amc_test = []\n",
    "    for id, line in enumerate(tqdm(test_set)):\n",
    "        amc_test.append({\n",
    "            'id': id, \n",
    "            'Question': line['problem'],\n",
    "            'answer': str(int(line['answer'])),\n",
    "            'url': line['url'],\n",
    "        })\n",
    "\n",
    "# Write the updated data to JSON\n",
    "with open(\"./AMC/test.json\", mode='w', encoding='utf-8') as json_file:\n",
    "    json.dump(amc_test, json_file, indent=4, ensure_ascii=False)\n",
    "print(len(amc_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data preprocess for LiveCodeBench\n",
    "Data Link: https://huggingface.co/datasets/livecodebench/code_generation_lite\n",
    "\"\"\"\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "def is_valid_date(date_str):\n",
    "    \"\"\"\n",
    "    Check if the given date string is within the range from August 1, 2024, to November 30, 2024.\n",
    "\n",
    "    Args:\n",
    "        date_str (str): The date string in the format \"%Y-%m-%dT%H:%M:%S\".\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the date is within the specified range, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse the date string into a datetime object\n",
    "        date = datetime.strptime(date_str, \"%Y-%m-%dT%H:%M:%S\")\n",
    "    except ValueError:\n",
    "        # If the date string is not in the expected format, consider it invalid\n",
    "        return False\n",
    "\n",
    "    # Define the start and end dates for the valid range\n",
    "    start_date = datetime(2024, 8, 1)\n",
    "    end_date = datetime(2024, 11, 30)\n",
    "\n",
    "    # Check if the date falls within the valid range\n",
    "    return start_date <= date <= end_date\n",
    "\n",
    "# Define the paths to the input JSONL files\n",
    "test_paths = [\n",
    "    './LiveCodeBench/test.jsonl',\n",
    "    './LiveCodeBench/test2.jsonl',\n",
    "    './LiveCodeBench/test3.jsonl',\n",
    "    './LiveCodeBench/test4.jsonl'\n",
    "]\n",
    "\n",
    "# Define the path to the output JSON file\n",
    "output_path = './LiveCodeBench/test.json'\n",
    "\n",
    "data_list = []\n",
    "seen_questions = set()  # To track unique questions based on 'question_content'\n",
    "current_id = 0  # To assign unique IDs across all files\n",
    "\n",
    "for test_path in test_paths:\n",
    "    try:\n",
    "        with open(test_path, 'r', encoding='utf-8') as file:\n",
    "            # Use tqdm to show progress; total can be estimated if needed\n",
    "            for line in tqdm(file, desc=f'Processing {test_path}'):\n",
    "                try:\n",
    "                    # Parse the JSON line\n",
    "                    line_data = json.loads(line)\n",
    "                except json.JSONDecodeError:\n",
    "                    # Skip lines that are not valid JSON\n",
    "                    continue\n",
    "\n",
    "                # Check if the 'contest_date' field exists and is valid\n",
    "                contest_date = line_data.get('contest_date')\n",
    "                if not contest_date or not is_valid_date(contest_date):\n",
    "                    continue\n",
    "\n",
    "                # Get the question content to check for duplicates\n",
    "                question_content = line_data.get('question_content')\n",
    "                if not question_content:\n",
    "                    continue  # Skip if 'question_content' is missing\n",
    "\n",
    "                if question_content in seen_questions:\n",
    "                    continue  # Duplicate question; skip\n",
    "\n",
    "                # Add the question to the seen set\n",
    "                seen_questions.add(question_content)\n",
    "\n",
    "                # Append the question data to the list\n",
    "                data_list.append({\n",
    "                    'id': current_id,\n",
    "                    'Question': question_content,\n",
    "                    'question_title': line_data.get('question_title', ''),\n",
    "                    'contest_date': contest_date,\n",
    "                    'difficulty': line_data.get('difficulty', ''),\n",
    "                    'public_test_cases': line_data.get('public_test_cases', [])\n",
    "                })\n",
    "\n",
    "                current_id += 1  # Increment the unique ID\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {test_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {test_path}: {e}\")\n",
    "\n",
    "# Write the aggregated and deduplicated data to the output JSON file\n",
    "try:\n",
    "    with open(output_path, mode='w', encoding='utf-8') as json_file:\n",
    "        json.dump(data_list, json_file, indent=4, ensure_ascii=False)\n",
    "    print(f\"Data successfully written to {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to write data to {output_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data preprocess for FlashRAG ODQA datasets\n",
    "Data Link: https://huggingface.co/datasets/RUC-NLPIR/FlashRAG_datasets\n",
    "\"\"\"\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_name = 'bamboogle'\n",
    "split = 'test'\n",
    "data_num = 500\n",
    "\n",
    "test_path = f'./FlashRAG_datasets/{dataset_name}/{split}.jsonl'\n",
    "output_path = f'./QA_Datasets/{dataset_name}.json'\n",
    "\n",
    "data_list = []\n",
    "with open(test_path, 'r') as file:\n",
    "    for id, line in enumerate(tqdm(file.readlines())):\n",
    "        line = json.loads(line)\n",
    "        data_list.append({\n",
    "            'id': id, \n",
    "            'Question': line['question'],\n",
    "            'answer': line[\"golden_answers\"],\n",
    "        })\n",
    "        if len(data_list) >= data_num:\n",
    "            break\n",
    "\n",
    "# Write the updated data to JSON\n",
    "with open(output_path, mode='w', encoding='utf-8') as json_file:\n",
    "    json.dump(data_list, json_file, indent=4, ensure_ascii=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uid_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
